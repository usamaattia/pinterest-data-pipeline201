{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-1209b9ad90a5-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/mount_name/topics/1209b9ad90a5.geo/partition=0/\"\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/mnt/mount_name/topics/1209b9ad90a5.geo/partition=0/topics/1209b9ad90a5.geo/partition=0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/mount_name/topics/1209b9ad90a5.geo/partition=0/topics/1209b9ad90a5.geo/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_geo = spark.read.format(file_type).option(\"inferSchema\", infer_schema).load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/mount_name/topics/1209b9ad90a5.geo/partition=0/topics/1209b9ad90a5.pin/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_pin = spark.read.format(file_type).option(\"inferSchema\", infer_schema).load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/mount_name/topics/1209b9ad90a5.geo/partition=0/topics/1209b9ad90a5.user/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_user = spark.read.format(file_type).option(\"inferSchema\", infer_schema).load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the df_pin DataFrame you should perform the following transformations:\n",
    "\n",
    "Replace empty entries and entries with no relevant data in each column with Nones\n",
    "Perform the necessary transformations on the follower_count to ensure every entry is a number. Make sure the data type of this column is an int.\n",
    "Ensure that each column containing numeric data has a numeric data type\n",
    "Clean the data in the save_location column to include only the save location path\n",
    "Rename the index column to ind.\n",
    "Reorder the DataFrame columns to have the following column order:\n",
    "ind\n",
    "unique_id\n",
    "title\n",
    "description\n",
    "follower_count\n",
    "poster_name\n",
    "tag_list\n",
    "is_image_or_video\n",
    "image_src\n",
    "save_location\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, expr\n",
    "\n",
    "\n",
    "\n",
    "# Replace empty entries and entries with no relevant data with Nones\n",
    "df_pin_cleaned = df_pin.na.replace('', None)\n",
    "\n",
    "# Perform transformations on follower_count column\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\", expr(\n",
    "    \"CASE WHEN follower_count LIKE '%k' THEN CAST(regexp_replace(follower_count, 'k', '') AS DOUBLE) * 1000 \" +\n",
    "    \"ELSE CAST(follower_count AS DOUBLE) END\"\n",
    ").cast(\"int\"))\n",
    "\n",
    "# Ensure numeric data columns have numeric data type\n",
    "numeric_columns = [\"follower_count\"]  # Add other numeric columns if needed\n",
    "for column in numeric_columns:\n",
    "    df_pin_cleaned = df_pin_cleaned.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "# Clean data in save_location column to include only the save location path\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"save_location\", col(\"save_location\").substr(23, 200))  # Assuming 23 is the starting index\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.na.replace('', None)\n",
    "\n",
    "# Rename the index column to 'ind'\n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "column_order = [\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"]\n",
    "df_pin_cleaned = df_pin_cleaned.select(column_order)\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_pin_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean the df_geo DataFrame you should perform the following transformations:\n",
    "\n",
    "Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "Drop the latitude and longitude columns from the DataFrame\n",
    "Convert the timestamp column from a string to a timestamp data type\n",
    "Reorder the DataFrame columns to have the following column order:\n",
    "ind\n",
    "country\n",
    "coordinates\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, array, to_timestamp\n",
    "\n",
    "# Create a new column 'coordinates' based on latitude and longitude columns\n",
    "df_geo_cleaned = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "# Drop the latitude and longitude columns from the DataFrame\n",
    "df_geo_cleaned = df_geo_cleaned.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Convert the 'timestamp' column from a string to a timestamp data type\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "column_order = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "df_geo_cleaned = df_geo_cleaned.select(column_order)\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_geo_cleaned.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean the df_user DataFrame you should perform the following transformations:\n",
    "\n",
    "Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "Drop the first_name and last_name columns from the DataFrame\n",
    "Convert the date_joined column from a string to a timestamp data type\n",
    "Reorder the DataFrame columns to have the following column order:\n",
    "ind\n",
    "user_name\n",
    "age\n",
    "date_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, to_timestamp\n",
    "\n",
    "# Create a new column 'user_name' by concatenating 'first_name' and 'last_name'\n",
    "df_user_cleaned = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# Drop the 'first_name' and 'last_name' columns from the DataFrame\n",
    "df_user_cleaned = df_user_cleaned.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert the 'date_joined' column from a string to a timestamp data type\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"date_joined\", to_timestamp(col(\"date_joined\")))\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "column_order = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "df_user_cleaned = df_user_cleaned.select(column_order)\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_user_cleaned.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most popular Pinterest category people post to based on their country.\n",
    "\n",
    "\n",
    "Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "country\n",
    "category\n",
    "category_count, a new column containing the desired query output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PinterestAnalytics\").getOrCreate()\n",
    "\n",
    "# Use window function to rank categories based on category_count\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(desc(\"category_count\"))\n",
    "\n",
    "# Create a DataFrame with the desired query output\n",
    "df_most_popular_category = df_pin_cleaned.join(\n",
    "    df_geo_cleaned.select(\"ind\", \"country\"),\n",
    "    \"ind\"\n",
    ").groupBy(\"country\", \"category\").count() \\\n",
    "    .withColumn(\"category_count\", col(\"count\")) \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"country\", \"category\", \"category_count\") \\\n",
    "    .orderBy(\"country\")\n",
    "\n",
    "# Show the result\n",
    "df_most_popular_category.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many posts each category had between 2018 and 2022.\n",
    "\n",
    "\n",
    "Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "post_year, a new column that contains only the year from the timestamp column\n",
    "category\n",
    "category_count, a new column containing the desired query output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PinterestAnalytics\").getOrCreate()\n",
    "\n",
    "# Join df_geo_cleaned and df_pin_cleaned based on 'ind'\n",
    "df_combined = df_geo_cleaned.join(\n",
    "    df_pin_cleaned.select(\"ind\", \"category\"),\n",
    "    \"ind\"\n",
    ")\n",
    "\n",
    "# Extract the year from the timestamp column\n",
    "df_posts_by_category = df_combined.withColumn(\"post_year\", year(col(\"timestamp\")))\n",
    "\n",
    "# Filter posts between 2018 and 2022\n",
    "df_posts_by_category = df_posts_by_category.filter((col(\"post_year\") >= 2018) & (col(\"post_year\") <= 2022))\n",
    "\n",
    "# Group by post_year and category to get category counts\n",
    "df_posts_by_category = df_posts_by_category.groupBy(\"post_year\", \"category\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"category_count\")\n",
    "\n",
    "# Show the result\n",
    "df_posts_by_category.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the user with the most follower in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max, first\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PinterestAnalytics\").getOrCreate()\n",
    "\n",
    "# Join df_pin_cleaned and df_geo_cleaned based on 'ind'\n",
    "df_combined = df_pin_cleaned.join(\n",
    "    df_geo_cleaned.select(\"ind\", \"country\"),\n",
    "    \"ind\"\n",
    ")\n",
    "\n",
    "# Step 1: For each country, find the user with the most followers\n",
    "df_max_followers_per_country = df_combined.groupBy(\"country\").agg(\n",
    "    col(\"country\"),\n",
    "    first(\"poster_name\").alias(\"poster_name\"),  # Using first() to get any value, as you want the most followers\n",
    "    max(\"follower_count\").alias(\"follower_count\")\n",
    ")\n",
    "\n",
    "# Show the result for Step 1\n",
    "df_max_followers_per_country.show()\n",
    "\n",
    "# Step 2: Find the country with the user having the most followers\n",
    "max_follower_entry = df_max_followers_per_country.orderBy(desc(\"follower_count\")).limit(1)\n",
    "\n",
    "# Show the result for Step 2\n",
    "max_follower_entry.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most popular category people post to based on the following age groups:\n",
    "\n",
    "18-24\n",
    "25-35\n",
    "36-50\n",
    "+50\n",
    "Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "age_group, a new column based on the original age column\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, dense_rank, desc\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PinterestAnalytics\").getOrCreate()\n",
    "\n",
    "# Create a new column 'age_group' based on the original 'age' column\n",
    "df_pin_cleaned = df_pin_cleaned.join(\n",
    "    df_user_cleaned.select(\"ind\", \"age\"),\n",
    "    \"ind\"\n",
    ").withColumn(\"age_group\", \n",
    "    when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "    .otherwise(\"+50\"))\n",
    "\n",
    "# Group by age_group and category to get category counts\n",
    "df_category_counts_by_age_group = df_pin_cleaned.groupBy(\"age_group\", \"category\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"category_count\")\n",
    "\n",
    "# Find the most popular category for each age group\n",
    "df_most_popular_category_by_age_group = df_category_counts_by_age_group \\\n",
    "    .withColumn(\"rank\", dense_rank().over(Window.partitionBy(\"age_group\").orderBy(desc(\"category_count\")))) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"age_group\", \"category\", \"category_count\")\n",
    "\n",
    "# Show the result\n",
    "df_most_popular_category_by_age_group.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the median follower count for users in the following age groups:\n",
    "\n",
    "18-24\n",
    "25-35\n",
    "36-50\n",
    "+50\n",
    "Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "age_group, a new column based on the original age column\n",
    "median_follower_count, a new column containing the desired query output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PinterestAnalytics\").getOrCreate()\n",
    "\n",
    "# Create a new column 'age_group' based on the original 'age' column\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"age_group\", \n",
    "                                           when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "                                           .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "                                           .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "                                           .otherwise(\"+50\"))\n",
    "\n",
    "# Group by age_group and calculate the median follower count using percentile_approx\n",
    "df_median_follower_count_by_age_group = df_pin_cleaned.groupBy(\"age_group\").agg(\n",
    "    col(\"age_group\"),\n",
    "    expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_median_follower_count_by_age_group.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find how many users have joined between 2015 and 2020.\n",
    "\n",
    "\n",
    "Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "post_year, a new column that contains only the year from the timestamp column\n",
    "number_users_joined, a new column containing the desired query output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PinterestAnalytics\").getOrCreate()\n",
    "\n",
    "# Assuming you have a DataFrame df_user_cleaned with cleaned user data\n",
    "# Columns: ind, user_name, date_joined\n",
    "\n",
    "# Extract the year from the 'date_joined' column\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "\n",
    "# Filter the data for the years 2015 to 2020\n",
    "df_user_joined_per_year = df_user_cleaned.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Group by post_year and count the number of users joined each year\n",
    "df_number_users_joined_per_year = df_user_joined_per_year.groupBy(\"post_year\").agg(\n",
    "    col(\"post_year\"),\n",
    "    count(\"user_name\").alias(\"number_users_joined\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_number_users_joined_per_year.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the median follower count of users have joined between 2015 and 2020.\n",
    "\n",
    "\n",
    "Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "post_year, a new column that contains only the year from the timestamp column\n",
    "median_follower_count, a new column containing the desired query output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, expr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PinterestAnalytics\").getOrCreate()\n",
    "\n",
    "# Extract the year from the 'timestamp' column in df_geo_cleaned\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"poster_year\", year(\"timestamp\"))\n",
    "\n",
    "df_combined = df_geo_cleaned.join(df_pin_cleaned, 'ind')\n",
    "\n",
    "# Filter the data for the years 2015 to 2020\n",
    "df_geo_joined_per_year = df_geo_cleaned.filter((col(\"poster_year\") >= 2015) & (col(\"poster_year\") <= 2020))\n",
    "\n",
    "# Group by post_year and calculate the median follower count using percentile_approx\n",
    "df_median_follower_count_per_year = df_combined.groupBy(\"poster_year\").agg(\n",
    "    col(\"poster_year\"),\n",
    "    expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_median_follower_count_per_year.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the median follower count of users that have joined between 2015 and 2020, based on which age group they are part of.\n",
    "\n",
    "\n",
    "Your query should return a DataFrame that contains the following columns:\n",
    "\n",
    "age_group, a new column based on the original age column\n",
    "post_year, a new column that contains only the year from the timestamp column\n",
    "median_follower_count, a new column containing the desired query output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, expr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PinterestAnalytics\").getOrCreate()\n",
    "\n",
    "# Join df_user_cleaned, df_pin_cleaned, and df_geo_cleaned on 'ind'\n",
    "df_joined = df_user_cleaned.alias(\"u\").join(df_pin_cleaned.alias(\"p\"), col(\"u.ind\") == col(\"p.ind\")).join(\n",
    "    df_geo_cleaned.alias(\"g\"), col(\"u.ind\") == col(\"g.ind\"))\n",
    "\n",
    "# Extract the year from the 'timestamp' column\n",
    "df_joined = df_joined.withColumn(\"post_year\", year(\"g.timestamp\"))\n",
    "\n",
    "# Define age groups based on the 'age' column from df_user_cleaned\n",
    "df_joined = df_joined.withColumn(\n",
    "    \"age_group\",\n",
    "    expr(\"CASE WHEN u.age BETWEEN 18 AND 24 THEN '18-24' \"\n",
    "         \"WHEN u.age BETWEEN 25 AND 35 THEN '25-35' \"\n",
    "         \"WHEN u.age BETWEEN 36 AND 50 THEN '36-50' \"\n",
    "         \"ELSE '+50' END\")\n",
    ")\n",
    "\n",
    "# Filter the data for the years 2015 to 2020\n",
    "df_joined_filtered = df_joined.filter((col(\"poster_year\") >= 2015) & (col(\"poster_year\") <= 2020))\n",
    "\n",
    "# Group by age_group, post_year, and calculate the median follower count using percentile_approx\n",
    "df_median_follower_count_per_age_group = df_joined_filtered.groupBy(\"age_group\", \"poster_year\").agg(\n",
    "    expr(\"percentile_approx(p.follower_count, 0.5)\").alias(\"median_follower_count\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_median_follower_count_per_age_group.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
