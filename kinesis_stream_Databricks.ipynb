{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-1209b9ad90a5-pin') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "df_pin = df.selectExpr(\"CAST(data as STRING)\")\n",
    "display(df_pin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pin.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"1209b9ad90a5_pin_table\")\n",
    "\n",
    "df_pin_cleaned = df_pin.na.fill(value=\"\")\n",
    "\n",
    "# Perform necessary transformations on follower_count column\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\", df_pin_cleaned[\"follower_count\"].cast(\"int\"))\n",
    "\n",
    "# Ensure that each column containing numeric data has a numeric data type\n",
    "numeric_columns = [\"follower_count\"]\n",
    "for col in numeric_columns:\n",
    "    df_pin_cleaned = df_pin_cleaned.withColumn(col, df_pin_cleaned[col].cast(\"int\"))\n",
    "\n",
    "# Clean the data in the save_location column\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"save_location\", F.split(df_pin_cleaned[\"save_location\"], \"/\").getItem(-1))\n",
    "\n",
    "# Rename the index column to ind\n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "desired_order = [\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"]\n",
    "df_pin_cleaned = df_pin_cleaned.select(desired_order)\n",
    "\n",
    "pin_table_path = \"/delta/1209b9ad90a5_pin_table\"\n",
    "\n",
    "# Write the DataFrame to Delta table\n",
    "df_pin_cleaned.write.format(\"delta\").mode(\"append\").save(pin_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-1209b9ad90a5-geo') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "df_geo = df.selectExpr(\"CAST(data as STRING)\")\n",
    "display(df_geo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"1209b9ad90a5_table_geo\")\n",
    "\n",
    "df_geo_cleaned = df_geo.withColumn(\"coordinates\", F.array(df_geo[\"latitude\"], df_geo[\"longitude\"]))\n",
    "\n",
    "# Drop the latitude and longitude columns\n",
    "df_geo_cleaned = df_geo_cleaned.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"timestamp\", df_geo_cleaned[\"timestamp\"].cast(\"timestamp\"))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "desired_order = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "df_geo_cleaned = df_geo_cleaned.select(desired_order)\n",
    "\n",
    "geo_table_path = \"/delta/1209b9ad90a5_geo_table\"\n",
    "\n",
    "# Write the DataFrame to Delta table\n",
    "df_geo_cleaned.write.format(\"delta\").mode(\"append\").save(geo_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-1209b9ad90a5-user') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "df_user = df.selectExpr(\"CAST(data as STRING)\")\n",
    "display(df_user)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"1209b9ad90a5_table_user\")\n",
    "\n",
    "# Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "df_user_cleaned = df_user.withColumn(\"user_name\", F.concat_ws(\" \", df_user[\"first_name\"], df_user[\"last_name\"]))\n",
    "\n",
    "# Drop the first_name and last_name columns\n",
    "df_user_cleaned = df_user_cleaned.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert the date_joined column from a string to a timestamp data type\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"date_joined\", df_user_cleaned[\"date_joined\"].cast(\"timestamp\"))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "desired_order = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "df_user_cleaned = df_user_cleaned.select(desired_order)\n",
    "\n",
    "user_table_path = \"/delta/1209b9ad90a5_user_table\"\n",
    "\n",
    "# Write the DataFrame to Delta table\n",
    "df_user_cleaned.write.format(\"delta\").mode(\"append\").save(user_table_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
